{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b224efde",
   "metadata": {},
   "source": [
    "# Sentiment (trying to understand NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969d603",
   "metadata": {},
   "source": [
    "We need to run this in the HPC as soon as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac1704a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "462e0cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets forget apple pay required brand new iphon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nz retailers don’t even contactless credit car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forever acknowledge channel help lessons ideas...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever go place doesn’t take apple pay doesn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay convenient secure easy use used kore...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241140</th>\n",
       "      <td>crores paid neerav modi recovered congress lea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241141</th>\n",
       "      <td>dear rss terrorist payal gawar modi killing pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241142</th>\n",
       "      <td>cover interaction forum left</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241143</th>\n",
       "      <td>big project came india modi dream project happ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241144</th>\n",
       "      <td>ever listen like gurukul discipline maintained...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240928 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Comment  Sentiment\n",
       "0       lets forget apple pay required brand new iphon...          1\n",
       "1       nz retailers don’t even contactless credit car...          0\n",
       "2       forever acknowledge channel help lessons ideas...          2\n",
       "3       whenever go place doesn’t take apple pay doesn...          0\n",
       "4       apple pay convenient secure easy use used kore...          2\n",
       "...                                                   ...        ...\n",
       "241140  crores paid neerav modi recovered congress lea...          0\n",
       "241141  dear rss terrorist payal gawar modi killing pl...          0\n",
       "241142                       cover interaction forum left          1\n",
       "241143  big project came india modi dream project happ...          1\n",
       "241144  ever listen like gurukul discipline maintained...          2\n",
       "\n",
       "[240928 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the sentiment dataset\n",
    "df = pd.read_csv('sentiment_data.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91919fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "2    103046\n",
       "1     82777\n",
       "0     55105\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count how many positive and negative reviews we have\n",
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186394f",
   "metadata": {},
   "source": [
    "We have a bunch more positive phrases than negative; is there enough reason to oversample the minority class? Look into the obtained confusion matrices and metrics.\n",
    "[30/06/2025]: After running tests 0 and 2 get different scores, so class imbalance is hurting the model a little bit, especially in the case of more complex algorithms. -> SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb345776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df['Comment']\n",
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eff1add3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 216835\n",
      "Number of unique words in the training dataset: 149124\n",
      "Number of features: 143649\n",
      "Number of training samples (from matrix size) from : 216835\n"
     ]
    }
   ],
   "source": [
    "#First simple approach: CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier    \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#Count the number of different words in the dataset\n",
    "print(f'Number of training samples: {len(X_train)}')\n",
    "print(f'Number of unique words in the training dataset: {len(set(\" \".join(X_train).split()))}')\n",
    "#Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "#Fit and transform the training data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "#Transform the test data\n",
    "X_test_vectorized = vectorizer.transform(X_test)   \n",
    "print(f'Number of features: {X_train_vectorized.shape[1]}')\n",
    "print(f'Number of training samples (from matrix size) from : {X_train_vectorized.shape[0]}')\n",
    "#SMOTE on training set [30/06/2025]\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_vectorized, y_train_smoted = smote.fit_resample(X_train_vectorized, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f41b0c",
   "metadata": {},
   "source": [
    "We can clearly see that the numbers match, number of unique words is very similar (should be about the same, processing of commas, exclamations aside) as features that each word vector has. Also, the number of training samples is exactly the same as the number of vectors in that vectorized matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ebbba",
   "metadata": {},
   "source": [
    "## Trying the Naive-Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064fbc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.71      0.64      5484\n",
      "           1       0.75      0.53      0.62      8337\n",
      "           2       0.71      0.80      0.75     10272\n",
      "\n",
      "    accuracy                           0.69     24093\n",
      "   macro avg       0.68      0.68      0.67     24093\n",
      "weighted avg       0.70      0.69      0.68     24093\n",
      "\n",
      "[[3920  494 1070]\n",
      " [1674 4386 2277]\n",
      " [1108  941 8223]]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#Create a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "#Fit the classifier on the training data\n",
    "classifier.fit(X_train_vectorized, y_train_smoted)\n",
    "#Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "#Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e6397",
   "metadata": {},
   "source": [
    "## Trying a more complex model: XGBoost (Random Forest Classifier in steroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7b5366",
   "metadata": {},
   "source": [
    "In this dataset, we can skip the outlier identification, it should be all correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701cee8",
   "metadata": {},
   "source": [
    "Now train the classifier with the best hyperparams. We applied optuna (put in in a cluster and obtained the next parameter results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20354eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [17:37:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Accuracy: 0.84\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.78      5484\n",
      "           1       0.81      0.89      0.84      8337\n",
      "           2       0.89      0.86      0.87     10272\n",
      "\n",
      "    accuracy                           0.84     24093\n",
      "   macro avg       0.84      0.83      0.83     24093\n",
      "weighted avg       0.84      0.84      0.84     24093\n",
      "\n",
      "[[4082  801  601]\n",
      " [ 396 7409  532]\n",
      " [ 486  991 8795]]\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=17,\n",
    "    learning_rate=0.7868971451620371,\n",
    "    subsample=0.823473166496611,\n",
    "    colsample_bytree=0.6167464902271917,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.6279386777664179,\n",
    "    reg_alpha=0.015410166345441269,\n",
    "    reg_lambda=0.3200061281054938,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "xgb_classifier.fit(X_train_vectorized, y_train_smoted)\n",
    "#Make predictions on the test data\n",
    "y_pred_xgb = xgb_classifier.predict(X_test_vectorized)\n",
    "#Evaluate the model\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f'XGBoost Classifier Accuracy: {accuracy_xgb:.2f}')\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6a6a1",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7a66bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy: 0.84\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.77      5484\n",
      "           1       0.85      0.83      0.84      8337\n",
      "           2       0.88      0.87      0.88     10272\n",
      "\n",
      "    accuracy                           0.84     24093\n",
      "   macro avg       0.83      0.83      0.83     24093\n",
      "weighted avg       0.84      0.84      0.84     24093\n",
      "\n",
      "[[4330  510  644]\n",
      " [ 861 6900  576]\n",
      " [ 640  673 8959]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create and train the neural network\n",
    "nn = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42, verbose = True)\n",
    "nn.fit(X_train_vectorized, y_train_smoted)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_nn = nn.predict(X_test_vectorized)\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "print(f\"Neural Network Accuracy: {accuracy_nn:.2f}\")\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "print(confusion_matrix(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f00a78ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6777/6777\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 34ms/step - accuracy: 0.7509 - loss: 0.6133 - val_accuracy: 0.8409 - val_loss: 0.4338\n",
      "Epoch 2/10\n",
      "\u001b[1m6777/6777\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 35ms/step - accuracy: 0.8603 - loss: 0.3890 - val_accuracy: 0.8512 - val_loss: 0.4149\n",
      "Epoch 3/10\n",
      "\u001b[1m6777/6777\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 35ms/step - accuracy: 0.8823 - loss: 0.3260 - val_accuracy: 0.8552 - val_loss: 0.4185\n",
      "Epoch 4/10\n",
      "\u001b[1m6777/6777\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 35ms/step - accuracy: 0.9070 - loss: 0.2621 - val_accuracy: 0.8551 - val_loss: 0.4336\n",
      "Epoch 5/10\n",
      "\u001b[1m6777/6777\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 35ms/step - accuracy: 0.9285 - loss: 0.2052 - val_accuracy: 0.8549 - val_loss: 0.4816\n",
      "Epoch 6/10\n",
      "\u001b[1m6777/6777\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 35ms/step - accuracy: 0.9482 - loss: 0.1537 - val_accuracy: 0.8529 - val_loss: 0.5357\n",
      "Epoch 7/10\n",
      "\u001b[1m6777/6777\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 35ms/step - accuracy: 0.9623 - loss: 0.1134 - val_accuracy: 0.8538 - val_loss: 0.6021\n",
      "Epoch 8/10\n",
      "\u001b[1m6777/6777\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 37ms/step - accuracy: 0.9721 - loss: 0.0851 - val_accuracy: 0.8521 - val_loss: 0.6824\n",
      "Epoch 9/10\n",
      "\u001b[1m6777/6777\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 37ms/step - accuracy: 0.9790 - loss: 0.0654 - val_accuracy: 0.8497 - val_loss: 0.7836\n",
      "Epoch 10/10\n",
      "\u001b[1m6777/6777\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 37ms/step - accuracy: 0.9836 - loss: 0.0508 - val_accuracy: 0.8490 - val_loss: 0.8480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29408fb10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "maxlen = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128),\n",
    "    LSTM(64),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1afe41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m753/753\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "LSTM Accuracy: 0.8489602789191881\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.80      5484\n",
      "           1       0.85      0.86      0.85      8337\n",
      "           2       0.87      0.87      0.87     10272\n",
      "\n",
      "    accuracy                           0.85     24093\n",
      "   macro avg       0.84      0.84      0.84     24093\n",
      "weighted avg       0.85      0.85      0.85     24093\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4304  545  635]\n",
      " [ 497 7170  670]\n",
      " [ 539  753 8980]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict class labels for the test set\n",
    "y_pred_lstm = model.predict(X_test_pad)\n",
    "y_pred_lstm_classes = y_pred_lstm.argmax(axis=1)  # Get the class with highest probability\n",
    "\n",
    "# Evaluate\n",
    "print(\"LSTM Accuracy:\", accuracy_score(y_test, y_pred_lstm_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lstm_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lstm_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ecdeaf",
   "metadata": {},
   "source": [
    "LSTM gave us some pretty amazing results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ca421",
   "metadata": {},
   "source": [
    "# LSTM + XGBoost ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72680d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m753/753\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "Ensemble Accuracy: 0.8659776698626157\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82      5484\n",
      "           1       0.86      0.88      0.87      8337\n",
      "           2       0.89      0.89      0.89     10272\n",
      "\n",
      "    accuracy                           0.87     24093\n",
      "   macro avg       0.86      0.86      0.86     24093\n",
      "weighted avg       0.87      0.87      0.87     24093\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4395  511  578]\n",
      " [ 410 7376  551]\n",
      " [ 457  722 9093]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Train both models (already done)\n",
    "# xgb_classifier.fit(X_train_vectorized, y_train_smoted)\n",
    "# model.fit(X_train_pad, y_train, ...)\n",
    "\n",
    "# 2. Get predicted probabilities for the test set\n",
    "proba_xgb = xgb_classifier.predict_proba(X_test_vectorized)  # shape: (n_samples, n_classes)\n",
    "proba_lstm = model.predict(X_test_pad)                       # shape: (n_samples, n_classes)\n",
    "\n",
    "# 3. Average the probabilities\n",
    "ensemble_proba = (proba_xgb + proba_lstm) / 2\n",
    "\n",
    "# 4. Get final predictions\n",
    "ensemble_pred = np.argmax(ensemble_proba, axis=1)\n",
    "\n",
    "# 5. Evaluate\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(\"Ensemble Accuracy:\", accuracy_score(y_test, ensemble_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, ensemble_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, ensemble_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2114b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
